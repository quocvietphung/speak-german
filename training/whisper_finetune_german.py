import os
from dataclasses import dataclass

import torch
from datasets import load_dataset, DatasetDict, Audio
from transformers import (
    WhisperProcessor,
    WhisperForConditionalGeneration,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
)
import evaluate

# (T√ôY CH·ªåN) N·∫øu v·∫´n OOM MPS, c√¢n nh·∫Øc m·ªü 1‚Äì2 d√≤ng d∆∞·ªõi (∆∞u ti√™n gi·∫£m batch/ƒë·ªô d√†i tr∆∞·ªõc):
# os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"
# os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"

# 1) Model & Processor
MODEL_ID = "openai/whisper-small"
LANG = "de"
TASK = "transcribe"

processor = WhisperProcessor.from_pretrained(MODEL_ID, language=LANG, task=TASK)
model = WhisperForConditionalGeneration.from_pretrained(
    MODEL_ID,
    attn_implementation="eager"  # tr√°nh SDPA ng·ªën RAM tr√™n MPS
)
model.config.use_cache = False     # gi·∫£m RAM khi train

# 2) Dataset (l·ªçc file <= 15s ƒë·ªÉ nh·∫π RAM I/O; v·∫´n pad l√™n 30s=3000 frame theo chu·∫©n Whisper)
common_voice = DatasetDict({
    "train": load_dataset("mozilla-foundation/common_voice_13_0", "de", split="train[:1%]"),
    "validation": load_dataset("mozilla-foundation/common_voice_13_0", "de", split="validation[:1%]"),
})
common_voice = common_voice.cast_column("audio", Audio(sampling_rate=16000))

MAX_SECONDS = 15
SR = 16000
def _dur_ok(example):
    return len(example["audio"]["array"]) <= SR * MAX_SECONDS

for split in ["train", "validation"]:
    common_voice[split] = common_voice[split].filter(_dur_ok)

# 3) Ti·ªÅn x·ª≠ l√Ω: t·∫°o input_features + labels (ƒë·ªÉ collator pad/truncate v·ªÅ 3000)
def preprocess(batch):
    audio = batch["audio"]
    inputs = processor(
        audio["array"],
        sampling_rate=audio["sampling_rate"],
        text=batch["sentence"],
        return_tensors="pt",
        padding=False,
    )
    batch["input_features"] = inputs.input_features[0]  # [80, T]
    batch["labels"] = inputs.labels[0]                  # token ids
    return batch

for split in ["train", "validation"]:
    common_voice[split] = common_voice[split].map(
        preprocess,
        remove_columns=common_voice[split].column_names,
        num_proc=1
    )

# 4) Collator: pad/truncate v·ªÅ ƒë√∫ng chu·∫©n Whisper [80, 3000]
@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: WhisperProcessor
    max_input_length: int = 3000   # Whisper y√™u c·∫ßu 3000 frame (=30s)
    max_label_length: int = 448

    def __call__(self, features):
        feats = []
        for f in features:
            feat = f["input_features"]
            if isinstance(feat, list):
                feat = torch.tensor(feat)
            elif not torch.is_tensor(feat):
                feat = torch.tensor(feat)

            T = feat.shape[-1]
            if T > self.max_input_length:
                feat = feat[:, : self.max_input_length]
            elif T < self.max_input_length:
                pad_T = self.max_input_length - T
                pad = torch.zeros(feat.size(0), pad_T, dtype=feat.dtype)
                feat = torch.cat([feat, pad], dim=-1)

            feats.append(feat)

        # [B, 80, 3000]
        input_features = torch.stack(feats, dim=0)

        # Pad labels t·ªõi max_label_length, r·ªìi mask -100
        label_dicts = [{"input_ids": f["labels"]} for f in features]
        labels_batch = self.processor.tokenizer.pad(
            label_dicts,
            padding="max_length",
            max_length=self.max_label_length,
            return_tensors="pt",
        )
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)

        # N·∫øu t·∫•t c·∫£ h√†ng b·∫Øt ƒë·∫ßu b·∫±ng BOS th√¨ b·ªè BOS ƒë·ªÉ tr√°nh t√≠nh loss
        bos_id = self.processor.tokenizer.bos_token_id
        if bos_id is not None and (labels[:, 0] == bos_id).all().item():
            labels = labels[:, 1:]

        return {
            "input_features": input_features,   # float tensor [B,80,3000]
            "labels": labels.long(),
        }

data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)

# 5) Metrics
wer_metric = evaluate.load("wer")

def compute_metrics(pred):
    pred_ids = pred.predictions
    label_ids = pred.label_ids
    label_ids = torch.where(
        torch.tensor(label_ids) == -100,
        torch.tensor(processor.tokenizer.pad_token_id),
        torch.tensor(label_ids)
    ).numpy()
    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)
    return {"wer": wer_metric.compute(predictions=pred_str, references=label_str)}

# 6) TrainingArguments ‚Äî T·∫ÆT gradient checkpointing ƒë·ªÉ tr√°nh l·ªói backward re-entrant
training_args = Seq2SeqTrainingArguments(
    output_dir="./whisper-small-de-test",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,    # gi·ªØ nh·ªè ƒë·ªÉ nhanh
    learning_rate=1.25e-5,
    warmup_steps=10,                  # ch·ªâ c·∫ßn v√†i b∆∞·ªõc warmup
    max_steps=100,                    # üöÄ ch·ªâ train 100 step
    eval_strategy="steps",
    eval_steps=50,                    # ƒë√°nh gi√° sau 50 step
    save_steps=50,
    logging_steps=10,
    predict_with_generate=True,
    gradient_checkpointing=False,
    dataloader_pin_memory=False,
    dataloader_num_workers=0,
    save_total_limit=1,
    report_to="none",
)

# 7) Trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=common_voice["train"],
    eval_dataset=common_voice["validation"],
    data_collator=data_collator,
    processing_class=processor,
    compute_metrics=compute_metrics,
)

# 8) Train
trainer.train()
metrics = trainer.evaluate()
print(metrics)

# -------- PH∆Ø∆†NG √ÅN B (n·∫øu mu·ªën checkpointing m√† v·∫´n an to√†n) ----------
# B·∫¨T l·∫°i GC theo ki·ªÉu non-reentrant:
# model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={"use_reentrant": False})
# v√† ƒë·ªïi training_args.gradient_checkpointing=False (gi·ªØ nguy√™n nh∆∞ tr√™n, KH√îNG b·∫≠t ·ªü args)
# Tham kh·∫£o PyTorch docs & HF issues v·ªÅ use_reentrant=False.  # noqa
