# ü§ñ 1. Import th∆∞ vi·ªán
import os
import torch
import evaluate
import numpy as np
from dataclasses import dataclass
from typing import Any, Dict, List

from datasets import load_dataset, Audio
from transformers import (
    WhisperProcessor,
    WhisperForConditionalGeneration,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    EarlyStoppingCallback  # th√™m EarlyStopping
)

# ü§ñ 2. C·∫•u h√¨nh ban ƒë·∫ßu
MODEL_ID = "openai/whisper-large-v3"
OUTPUT_DIR = "./training/models/whisper_de_finetune"

# Load dataset Common Voice 13.0 (ti·∫øng ƒê·ª©c)
common_voice_train = load_dataset("mozilla-foundation/common_voice_13_0", "de", split="train+validation")
common_voice_eval = load_dataset("mozilla-foundation/common_voice_13_0", "de", split="test")

# Chuy·ªÉn audio sampling_rate v·ªÅ 16kHz cho ph√π h·ª£p v·ªõi Whisper
common_voice_train = common_voice_train.cast_column("audio", Audio(sampling_rate=16000))
common_voice_eval = common_voice_eval.cast_column("audio", Audio(sampling_rate=16000))

# ü§ñ 3. Load processor v√† model
processor = WhisperProcessor.from_pretrained(MODEL_ID, language="de", task="transcribe")
model = WhisperForConditionalGeneration.from_pretrained(MODEL_ID)

# ƒê√≥ng bƒÉng encoder ƒë·ªÉ gi·∫£m s·ªë tham s·ªë ph·∫£i train (tƒÉng t·ªëc v√† tr√°nh overfitting)
model.freeze_encoder()

# ü§ñ 4. H√†m preprocess d·ªØ li·ªáu (chu·∫©n b·ªã input_features v√† labels)
def prepare_dataset(batch):
    audio = batch["audio"]
    # T√≠nh feature t·ª´ audio
    batch["input_features"] = processor.feature_extractor(audio["array"], sampling_rate=16000).input_features[0]
    # M√£ h√≥a c√¢u tho·∫°i (sentence) th√†nh token id ƒë·ªÉ l√†m nh√£n
    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch

# Map h√†m preprocess v√†o dataset
train_dataset = common_voice_train.map(prepare_dataset, remove_columns=common_voice_train.column_names, num_proc=4)
eval_dataset = common_voice_eval.map(prepare_dataset, remove_columns=common_voice_eval.column_names, num_proc=4)

# ü§ñ 5. Data collator t√πy bi·∫øn cho Seq2Seq (padding cho audio v√† text)
@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: WhisperProcessor

    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:
        # T√°ch input_features v√† labels
        input_features = [{"input_features": f["input_features"]} for f in features]
        label_features = [{"input_ids": f["labels"]} for f in features]

        # Padding cho input_features (audio) v√† chuy·ªÉn sang tensor
        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")
        # Padding cho labels (text) v√† chuy·ªÉn sang tensor
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")

        # Thay token padding b·∫±ng -100 ƒë·ªÉ kh√¥ng t√≠nh v√†o loss
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)
        batch["labels"] = labels
        return batch

data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)

# ü§ñ 6. ƒê·ªãnh nghƒ©a metric WER (Word Error Rate)
wer_metric = evaluate.load("wer")

def compute_metrics(pred):
    pred_ids = pred.predictions
    label_ids = pred.label_ids

    # Gi·∫£i m√£ chu·ªói d·ª± ƒëo√°n v√† nh√£n th·∫≠t v·ªÅ text
    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id  # b·ªè qua -100
    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)

    # T√≠nh WER b·∫±ng th∆∞ vi·ªán evaluate
    wer = wer_metric.compute(predictions=pred_str, references=label_str)
    return {"wer": wer * 100}  # nh√¢n 100 ƒë·ªÉ bi·ªÉu th·ªã d∆∞·ªõi d·∫°ng %
    # (WER c√†ng th·∫•p c√†ng t·ªët, 0% l√† ho√†n h·∫£o)

# ü§ñ 7. Thi·∫øt l·∫≠p tham s·ªë hu·∫•n luy·ªán
training_args = Seq2SeqTrainingArguments(
    output_dir=OUTPUT_DIR,
    # Batch size (c√≥ th·ªÉ gi·∫£m n·∫øu OOM, Apple M3 kh√¥ng c√≥ GPU CUDA n√™n ch·∫°y tr√™n CPU/MPS)
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    gradient_accumulation_steps=2,  # t√≠ch l≈©y gradient cho batch hi·ªáu d·ª•ng = 16
    # Chi·∫øn l∆∞·ª£c ƒë√°nh gi√° & l∆∞u m√¥ h√¨nh
    evaluation_strategy="epoch",       # ƒë√°nh gi√° m·ªói epoch
    save_strategy="epoch",             # l∆∞u checkpoint m·ªói epoch
    load_best_model_at_end=True,       # t·ª± ƒë·ªông load model t·ªët nh·∫•t cu·ªëi training
    metric_for_best_model="wer",       # d√πng WER tr√™n eval ƒë·ªÉ ch·ªçn model t·ªët nh·∫•t
    greater_is_better=False,           # WER c√†ng th·∫•p c√†ng t·ªët
    save_total_limit=2,                # ch·ªâ l∆∞u t·ªëi ƒëa 2 checkpoint (ti·∫øt ki·ªám dung l∆∞·ª£ng)
    # Thi·∫øt l·∫≠p optimizer v√† l·ªãch h·ªçc
    learning_rate=1e-5,
    warmup_steps=500,
    num_train_epochs=5,               # train t·ªëi ƒëa 5 epoch (c√≥ th·ªÉ d·ª´ng s·ªõm n·∫øu kh√¥ng c·∫£i thi·ªán)
    # K·ªπ thu·∫≠t h·ªó tr·ª£ training
    gradient_checkpointing=True,      # d√πng gradient checkpointing ƒë·ªÉ ti·∫øt ki·ªám b·ªô nh·ªõ
    fp16=torch.cuda.is_available(),   # d√πng FP16 n·∫øu c√≥ GPU CUDA (MPS ch∆∞a h·ªó tr·ª£ FP16 t·ªët)
    max_grad_norm=1.0,                # gradient clipping v·ªõi norm t·ªëi ƒëa 1.0
    # Logging
    logging_steps=50,
    report_to=["tensorboard"],        # log ra tensorboard ƒë·ªÉ tr·ª±c quan n·∫øu c·∫ßn
    eval_accumulation_steps=1,        # t√≠nh metric tr√™n to√†n b·ªô t·∫≠p eval m·ªôt l∆∞·ª£t
    predict_with_generate=True,       # generate text trong eval ƒë·ªÉ t√≠nh WER
    push_to_hub=False                 # kh√¥ng push l√™n Hugging Face Hub trong qu√° tr√¨nh train
)

# ü§ñ 8. Kh·ªüi t·∫°o Trainer v·ªõi c√°c th√†nh ph·∫ßn c·∫ßn thi·∫øt
trainer = Seq2SeqTrainer(
    args=training_args,
    model=model,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    data_collator=data_collator,
    tokenizer=processor.tokenizer,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.0)]
    # EarlyStopping: n·∫øu WER kh√¥ng c·∫£i thi·ªán sau 3 l·∫ßn eval (3 epoch li√™n ti·∫øp) th√¨ d·ª´ng
)

# ü§ñ 9. B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán
trainer.train()

# ü§ñ 10. L∆∞u model v√† tokenizer ƒë√£ fine-tune
trainer.save_model(OUTPUT_DIR)
processor.save_pretrained(OUTPUT_DIR)

# ü§ñ 11. Tr·ª±c quan h√≥a k·∫øt qu·∫£ hu·∫•n luy·ªán (v·∫Ω ƒë·ªì th·ªã Loss v√† WER)
import matplotlib.pyplot as plt

history = trainer.state.log_history  # l·ªãch s·ª≠ log

# L·∫•y c√°c gi√° tr·ªã loss v√† WER t·ª´ l·ªãch s·ª≠ log
train_steps = [entry["step"] for entry in history if "loss" in entry and "step" in entry]
train_loss = [entry["loss"] for entry in history if "loss" in entry]
eval_steps = [entry["step"] for entry in history if "eval_loss" in entry]
eval_loss = [entry["eval_loss"] for entry in history if "eval_loss" in entry]
eval_wer = [entry["eval_wer"] for entry in history if "eval_wer" in entry]

# V·∫Ω bi·ªÉu ƒë·ªì Train vs Val Loss
plt.figure(figsize=(6,4))
plt.plot(train_steps, train_loss, label="Train loss")
plt.plot(eval_steps, eval_loss, label="Validation loss")
plt.xlabel("Step")
plt.ylabel("Loss")
plt.title("Training & Validation Loss")
plt.legend()
plt.show()

# V·∫Ω bi·ªÉu ƒë·ªì WER tr√™n t·∫≠p Validation
plt.figure(figsize=(6,4))
plt.plot(eval_steps, eval_wer, marker='o', color='orange')
plt.xlabel("Step")
plt.ylabel("WER (%)")
plt.title("Validation WER")
plt.show()
